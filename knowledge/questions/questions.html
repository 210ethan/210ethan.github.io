<!DOCTYPE html>
<html>
<head>
  <title>Questions</title>
  <link rel="stylesheet" type="text/css" href="../../css/main.css">
  <link rel="icon" href="../../media/ethan_morse_favicon.png">
</head>

<body>

<h3 id="home"><a href="../../index.html">Home</a></h3>

<h1 id="questions">Questions</h1>

<p>
  This is a place for me to post answers to questions that I think of.
</p>

<hr>

<ul>
  <li>
    <b>How do antibodies form and work?</b> The following answers are from Pacific Immunology's <a href+"https://www.pacificimmunology.com/resources/antibody-introduction/what-is-an-antibody/">Antibody Introduction series</a>. First, "An antibody is a protein produced by the immune system that is capable of binding with high specificity to an antigen", where the antigen is another protein, carbohydrate, molecule, or nucleotide. To produce an antibody, specialized cells (macrophages and dendritic) capture the foreign molecule, break it down, and present the antigen to B cell lymphocytes. <a href="https://en.wikipedia.org/wiki/Somatic_hypermutation">Somatic hypermutation</a> then occurs, where the B cell searches for the complementary antibody that will uniquely bond to the antigen's epitope. When successful, the antibody is released into the bloodstream to bind with and eliminate the antigens from the system.
  </li>
  <li>
    <b>How much U.S. currency is in circulation?</b> <a href="https://www.federalreserve.gov/faqs/currency_12773.htm">$2.01 trillion</a>.
  </li>
  <li>
    <b>Are there any non-linear recipes?</b> cooking.stackexchange answer <a href="https://cooking.stackexchange.com/questions/28216/are-there-any-recipe-ingredients-that-scale-in-a-non-uniform-manner">here</a>:
    <blockquote>
      Things applied in a non-linear manner do not scale linearly, i.e. when the 'Surface to volume ratio' matters, the recipe will not scale linearly. See http://kitchenscience.sci-toys.com/scaling for a discussion mostly on how the timings are affected. One example is breading: You will not need to double the breading linearly on a single 200g piece of beef/chicken compared to a single 400g piece, since the surface area will not change with the same factor as the weight, i.e. the surface area will not double when the weight doubles. On the other hand, if you use twice as many pieces of chicken, you will need twice as much breading.
    </blockquote>
  </li>
  <li>
    <b>I just saw the news regarding AlphaFold. How big of a breakthrough do you think this is? In some online tech circles I’m a part of, some users are saying it’s a major event, while others are calling it overhyped. I’d appreciate your perspective as a professor.</b> AlphaFold is indeed a breakthrough, based on its recent performances in annual CASP, in applying deep learning in biology. Of course, it needs to be tested by time on its potential of actually designing new proteins with desired properties. But I would consider it significant as at least it outperformed many existing methods, for which researchers have spent significant efforts and time to develop.
  </li>
  <li>
    <b>What would the software structure of an AGI look like? Just a collection of individual files like this chatbot? I understand the hardware needed is immense, but not sure about the software side of things.</b> This was asked in r/TheMotte's Small-Scale Question Sunday.
    <blockquote>
      "Software structure" exists to benefit human developers. An AGI, at least within the current paradigms of AI, would be self-evolved through enormous amounts of unsupervised learning. The resulting software artefact would be "written" by and for a machine, so to begin with not really "written" in a programming language, as a such brings no benefit to the machine. So it would look like a very large obfuscated compiled binary, but it would be even harder for humans to decompile and reverse-engineer than such binaries usually are (which is very), because it hasn't been written by a human whose intentions could be hypothesised in order to work out the structure.
    </blockquote>
    <blockquote>
      Probably a giant spreadsheet of weights encoding a giant neural net. What this net would run on is much less important, as long as self-modification is supported.
    </blockquote>
  </li>
  <li>
    <b>Why does the return trip seem shorter?</b> From this <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3179583/">paper</a>:
    <blockquote>
      Instead, the return trip effect is likely due to a violation of expectations. Participants felt that the initial trip took longer than they had expected. In response, they likely lengthened their expectations for the return trip. In comparison with this longer expected duration, the return trip felt short. The greater the participants’ expectations were violated on the initial trip, the more they experienced the return trip effect
    </blockquote>
  </li>
  <li>
    <b>Why is rhyming so pleasing?</b> I wasn't able to find any literature, but it appears Reddit's consensus is that our brain likes patterns and rewards use for predicting them.
  </li>
  <li>
    <b>Is chewing food more better for digestion?</b> Yes, smaller chunks of food allow the stomach to more easily process it.
  </li>
  <li>
    <b>Is dying from a broken heart a real thing?</b> Yes, see <a href="https://www.health.harvard.edu/heart-health/takotsubo-cardiomyopathy-broken-heart-syndrome">takotsubo cardiomyopathy (broken-heart syndrome)</a>.
  </li>
  <li>
    <b>What resolution do our eyes see at?</b> <a href="https://clarkvision.com/articles/eye-resolution.html">According to Roger Clark</a>, around 576 Mpx.
  </li>
</ul>

<hr>

<p>
  I asked this question on r/TheMotte's <a href="https://www.reddit.com/r/TheMotte/comments/iamulv/smallscale_question_sunday_for_the_week_of_august/g1q8o9j?utm_source=share&utm_medium=web2x">Small-Scale Question Sunday for the week of August 16, 2020</a>:
</p>

<blockquote>
  <p>
    How do I get better at determining if a paper's methods and results are reliable and robust? Often when reading a paper (mostly social sciences), I find myself agreeing with their methods and findings, only to find someone who is seemingly qualified rip the methods and results to threads.
  </p>
  <p>
    My probability and statistics backgrounds are average and rudimentary, respectively, which I think has something to do with it. Would learning more about statistics/experimental methods be the best course of action? If so, any suggestions on resources (textbooks, lectures, etc.)? DeGroot and Schervish's Probability and Statistics seems reputable, but I'm open to other recommendations.
  </p>
</blockquote>

<p>
  The answers I got were (each blockquote is an individual user's answer):
</p>

<blockquote>
  <ol>
    <li>
      Look for confounding effects and whether they were controlled for. All good papers should mention them -- if they don't, it's an indication of agenda. For example, hours worked for the 'earnings gap', power in 'interrupting', Socio-economic-status indications in name studies.
    </li>
    <li>
      You can pretty much dismiss studies that use self-selection (an internet survey) or self-report (e.g. dating preferences).
    </li>
    <li>
      p levels near 0.05 and low or unreported effect sizes are suspicious (but this seems a rarer form of deception).
    </li>
  </ol>
</blockquote>

<blockquote>
  your question remind me of a paul graham quote, which i think is very much applicable to academic papers - and my opinion is that it's more likely to be useful than digging into probability and statistics (although, that's not a waste of time): "Remember the exercises in critical reading you did in school, where you had to look at a piece of writing and step back and ask whether the author was telling the whole truth? If you really want to be a critical reader, it turns out you have to step back one step further, and ask not just whether the author is telling the truth, but why he's writing about this subject at all." -- Paul Graham
</blockquote>

<blockquote>
  Some approaches I take:

  <ol>
    <li>
      Think about how the results could have been achieved by something other than the proposed mechanism. Look for what is unobserved, much of which will be due to unobservables. For example, if cancer rates are higher in (say) soda drinkers, one could easily craft an explanation that there's some general propensity for unhealthy behavior that causes both soda drinking and other unhealthy eating. Or that poverty correlates with soda consumption and exposure to lead paint.
    </li>
    <li>
      Related to (1): think about causal diagrams. LessWrong has a long description here. The most simplified case: if A and B are correlated, then A could cause B, B could cause A, or some unobserved C could cause A and B.
    </li>
    <li>
      Look not just at statistical significance, but practical significance. You can have significant p-values with effect sizes nobody would care about.
    </li>
    <li>
      For studies demonstrating negatives, think about sample and effect sizes. It's very hard to demonstrate that something causes a low-probability event (like death or cancer) without huge sample size, so finding "no evidence for" low-probability events is easy.
    </li>
    <li>
      Critically evaluate the incentives of the researcher and how they might come through in the published findings. Even with a large-sample-size randomized controlled trial, researchers have broad latitude over which effects to evaluate, whether outliers get trimmed, which subgroups to evaluate, and whether they publish their studies at all. If you see that data collection stopped early, that a drug impacts skin cancer rates when it could've just have easily been tested on 20 other cancers, that an effect is found in men 18-24 but in no other subgroup, etc., this is evidence for p-hacking.
    </li>
    <li>
      Evaluate selection bias. Is there something fundamental in how people were chosen for a study that makes it non-random to begin with? Would they attrit from a study in a non-random way?
    </li>
    <li>
      Look for confirming or contradictory research. Even if there isn't anything studying the exact topic, you can do this in a Bayesian fashion - there may be something that you'd have some level of confidence in generalizing to the topic at hand. For the soda example, if soda is found to be carcinogenic in only one human study and you can find a few other studies showing that the individual ingredients (artificial colors, phosphoric acid) are carcinogenic in mice, this is some level of evidence for their claim.
    </li>
  </ol>
</blockquote>

<blockquote>
  <p>
    Honestly? You can't. Not without becoming an expert in the field. From the outside, you could spot egregiously bad statistical errors or total failures of control, but those will be the minority of cases. What look like unfounded assumptions may simply be textbook knowledge so universal in the field and so well replicated that nobody even bothers citing it anymore. Conversely, what seems utterly unremarkable may be a fatal flaw.
  </p>
  <p>
    I recall one paper I peer reviewed. I loved this paper, right up my alley in an area I was interested in, done by people I know and respect. Everything was great...except they used one wrong drug which tainted all of the results. The number of people who are in a situation to use this drug *and* study the result topic which it damages is tiny, maybe 20 of us in the world. But that was it, the study was dead. To the authors' credit, they didn't re-submit and I've never seen the paper anywhere else, so I think they just tossed it. And they were new to the field, so they simply didn't know.
  </p>
  <p>
    There are a lot of downsides to peer review, but the biggest upside, and it's a HUGE one, is that these people have deep knowledge of the topic and have often uses these same experimental techniques themselves. They know the quirks, the weird confounders, that one study from an obscure journal 20 years ago that proves you can't study X with this technique even if it works for all the other letters, but also the difficulties and realistic limitations (e.g. working with dangerous chemicals or endangered species).
  </p>
</blockquote>

<blockquote>
  Learning more about stats wouldn't hurt, but another good approach is reading a lot of teardowns of studies. Go through Gelman's blog archives, for example. There are classes of methodological errors that keep coming up over and over again, and even if you can't identify them on your own the first time you see each one, you can learn to recognize them once you've seen an example or two.
</blockquote>

<hr>

<ul>
  <li>
    <b>What makes some materials more flammable than others?</b> From <a href="https://chemistry.stackexchange.com/questions/87322/why-are-some-materials-more-flammable-than-others">chemistry.stackexchange</a>: "for a material to be flammable it needs to contain something that is oxidizable by air oxygen, typically carbon ... The more volatile a material is, the more flammable it will be." Volatility is determined by the material's <a href="https://en.wikipedia.org/wiki/Volatility_(chemistry)#Intermolecular_forces">intermolecular forces</a>.
  </li>
  <li>
    <b>What determines blood types?</b> The four major blood groups—A, B, O, AB—each have a protein, the <a href="https://en.wikipedia.org/wiki/Rh_factor">Rh factor</a>, that is either present or absent. This creates 8 blood types: A+, A-, ..., AB-. The A and B refer to specific <a href="https://en.wikipedia.org/wiki/Antigen">antigen</a>. The letter refers to the present antigen, while the opposite letter's antibody is present. For example, type A+ has an A antigen, B antibody, and present Rh factor. Group O has neither antigens but both antibodies. Universal red cell donor is O- and universal plasma donor is AB (+ or -). Source <a href="https://www.redcrossblood.org/donate-blood/blood-types.html">here</a>.
  </li>
  <li>
    <b>How are future populations determined?</b> From the <a href="https://www.census.gov/programs-surveys/popproj/about.html">U.S. Census Bureau</a>: "population projections are based on the most recent decennial census and using the cohort-component method (see bottom of link for definition). Assumptions about future births, deaths, net international migration, and domestic migration" are also used. Multiple projections may also be used, where each projection falls into a range based on the assumptions' ranges. This <a href="https://assets.prb.org/pdf14/understanding-population-projections.pdf">paper</a> describes the major assumptions (fertility, mortality, migration) used in creating populatio projections.
  </li>
  <li>
    <b>What determines the seasonality of a virus?</b> The truth is we don't entirely know. Some diseases increase in the summer, others in the winter. Knowing the functions behind the seasonality will help us combat future viruses.
  </li>
  <li>
    <b>What is PGP and how does it work?</b> PGP stands for <a href="https://en.wikipedia.org/wiki/Pretty_Good_Privacy">Pretty Good Privacy</a>. Good explanation <a href="https://www.varonis.com/blog/pgp-encryption/">here</a>.
  </li>
</ul>

























</body>
</html>
